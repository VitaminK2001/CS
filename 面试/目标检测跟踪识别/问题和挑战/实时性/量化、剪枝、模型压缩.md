# Quantization
将深度学习模型中的参数和激活值从浮点数转换为较低位宽的固定点数或整数表示
## 二值量化（Binary Quantization）
将权重和激活值量化为二进制数（通常是+1和-1）
```python
import numpy as np
def binary_quantization(x):
    return np.sign(x)
```
## 线性量化（Linear Quantization）
将权重和激活值量化为一组离散的值，通常在一定范围内均匀分布。
```python
def linear_quantization(x, num_bits=8):
    scale = (2 ** num_bits - 1) / (np.max(x) - np.min(x))
    return np.round(x * scale)
```
## 对称量化（Symmetric Quantization）
在量化过程中保持权重和激活值的正负对称性，通常与线性量化结合使用。
```python
def symmetric_quantization(x, num_bits=8):
    scale = (2 ** (num_bits - 1) - 1) / max(np.abs(np.min(x)), np.abs(np.max(x)))
    return np.round(x * scale)
```
## 非线性量化（Non-Linear Quantization）
使用非线性函数进行量化，==例如==使用sigmoid函数
```python
def sigmoid_quantization(x):
    return 1 / (1 + np.exp(-x))
```

## 常用量化
### 对称线性量化（Symmetric Linear Quantization）

对称线性量化是一种常见的量化方法，特别适用于需要保持权重和激活值的正负对称性的情况，如卷积神经网络（CNN）和循环神经网络（RNN）等模型。
==较好地保持数据的分布特性，维持了正负权重的平衡，有助于减小量化误差，硬件实现简单==

### 非线性量化（Non-Linear Quantization）
非线性量化通常用于特定需求，例如保持数据的分布特性或优化模型的某些方面，如使用 sigmoid 或 tanh 函数进行量化。
==可能引入一些非线性的变换，导致量化误差相对较大==

# Pruning
删除模型中不必要的权重或神经元来减小模型大小
## Weight Pruning
在训练后更具权重的绝对值大小去除不必要的连接
## Channel Pruning
针对CNN的卷积通道，适用于减小模型的通道数量  [[通道、卷积核神经元、特征图]]
## Structured Pruning
结构剪枝允许剪枝整个神经元或卷积核。这种方法在需要更进一步减小模型大小、提高模型压缩率的情况下使用。结构剪枝通常在训练后应用。


# Model Compression
综合性的技术，通过各种手段减小深度学习模型的大小和计算开销

在资源有限的嵌入式设备或需要实时推理的应用中