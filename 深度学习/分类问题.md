# binary classification
there being only two possible categories (no yes / false true / zero one)
线性回归不光预测零和一，而是预测所有介于零和一或者小于零大于一的值
现在要预测类别，所以可以通过确定阈值(threshold value)的方式
但是这样做的问题在于，如果添加样例，可能会影响分类的结论
![[Pasted image 20221223160526.png]]
the best fit line(拟合线) 
the deviding line(decision boundary)决策边因为拟合线的移动而移动
导致添加样例时可能产生更差的拟合线 

## logistic regression 逻辑回归
注意：虽然名字中有回归这两个字，但是实际上用于分类问题，而且是用于解决01分类问题
![[Pasted image 20221223161236.png|700]]
### 逻辑回归函数：
![[Pasted image 20221223161709.png|575]]

## sigmoid function
![[Pasted image 20221223161529.png|575]]
通过sigmoid function算出来的值我们**当做是y = 1的概率**
因为sigmoid函数在z = 0时输出是0.5如果将0.5作为阈值，
则决策边的形状是根据z = 0时不同特征值的值画出来的

## 训练逻辑回归模型
### 逻辑回归中的代价函数
![[Pasted image 20221225122840.png]]
如果代价函数选择的是平方误差函数则代价函数和w,b构成的图形不是凸函数的图形
所以平方误差函数的选择是不好的，因为可能陷入局部最优解
![[Pasted image 20221225123032.png]]
所以最终的代价函数是根据真实的y而言的，如果真实的y是1，那么预测值f越靠近1损失函数越小
同理，真实的y是0，预测值f越靠近0损失函数越小
所以有根据y的不同的两个不同的损失函数cost function
![[Pasted image 20221225124306.png]]
这个代价函数得到的图像是凸函数的，便于找到全局最优解
#### 代价函数的简化
因为y = 0的时候只剩下后半部分，y = 1的时候只剩下前半部分，所以简化如下
![[Pasted image 20221225124722.png]]






