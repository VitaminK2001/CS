# regression 回归问题
为一个或多个自变量与因变量之间关系建模的一类方法，常用来表示输入和输出之间的关系。
通常预测任务会涉及到回归问题，但不是所有预测都是回归问题（如分类问题）。
## 有监督学习
训练数据有标签
分类问题和回归问题
本质学习输入输出之间的映射关系
回归（定量）分类（定性）
区别是变量离散化与否

## 无监督学习
训练数据无标签
聚类问题 clustering

# 一元特征线性回归模型
## cost function
通过 gradient descent
找到最小的cost function 
linear regression 的 squared error cost function 是一个3D碗状图
因为cost function是预测值y-hat 和 实际值y间差值的平方和
通过控制$f_{w,b}(x)$中w，b两个变量
找到最小的差值

### 梯度下降算法
repeat until convergence
w = w - alpha * cost function'(w)
b = b - alpha * cost function'(b) 
updaet w and b simultaneously at each step

converge(收敛)
diverge(发散)
### 线性回归算法
mean squarred error cost function + gradient descent = linear regression algorithm
#### 选择方差损失函数的原因
squarred error cost function has a single global minimum because of this bowl shape            
convex function(凸函数) don't have any local minima

# 多元线性回归模型
多元：行向量
$x^{(i)}_{j}$ : **jth** feature in **ith** training example
multiple linear regression
## vectorization 向量化
向量化可以用来GPU加速
### parameters and features
$\vec {w} = [w1, w2, w3]$
b is a number
$\vec {x} = [x1, x2, x3]$
numerical linear algebra libarary  -- numpy
np.array (create)
np.dot (点积)
如果手动dot则无法调用硬件并行计算，时间就会变慢
假设 
w = np.array([0.5, 1.3, ..., 3.4])
d = np.array([0.3, 0.2, ..., 0.4])
compute wj = wj - 0.1dj for j = 1...16
同样，如果没有矢量化，则要写for循环
```python
for j in range(0, 16):
	w[j] = w[j] - 0.1 * d[j]
```
如果有矢量化则
```python
w = w-0.1*d
```

## gradient descent for multiple regression
![[Pasted image 20221216192330.png]]

# 特征缩放
dividing by the maximum 除以最大值

mean normalization 均值归一化 
dividing by max-min

z-score normalization 标准差
x - 平均值(mean) / 标准差

特征缩放的目的是为了让梯度下降运行的更快

# 设置学习率
![[截屏2022-12-17 10.09.58.png]]

# 多项式回归 （线性回归的变式）
![[Pasted image 20221223155423.png]]

如果要用梯度下降，就需要用到特征缩放，因为x的范围会因为cube的原因而扩大













